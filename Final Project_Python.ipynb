{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Pulling tweet data from Titter API.\n",
    "I separated this out so I could run it a few different ways. \n",
    "\n",
    "**Import libraries and connect to Twitter API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries needed to pull tweets, and establish connection to twitter api\n",
    "import json\n",
    "import tweepy\n",
    "\n",
    "%run ~/twitter_credentials.py\n",
    "\n",
    "#Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "#Connect to the Twitter API using the authentication. have it wait on rate limits and notify when it's waiting\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull tweets using the Twitter REST API based on a specified hashtag**\n",
    "\n",
    "The next set of code was used to collect tweets based on different hashtags.  I wanted the data separate by hashtags so that I could analyze it more and determine if I needed all of them, or if a subset would be enough.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pull tweets based on a specific hastag\n",
    "def get_tweets_by_hashtag(hashtag, num_tweets, filename):\n",
    "    num_needed = num_tweets\n",
    "    tweet_list = []\n",
    "    last_id = -1 # id of last tweet seen\n",
    "    \n",
    "    while len(tweet_list) < num_needed:\n",
    "        try:\n",
    "            new_tweets = api.search(q = hashtag, count = 100, max_id = str(last_id - 1))\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Error\", e)\n",
    "            break\n",
    "        else:\n",
    "            if not new_tweets:\n",
    "                print(\"Could not find any more tweets!\")\n",
    "                break\n",
    "            tweet_list.extend(new_tweets)\n",
    "            last_id = new_tweets[-1].id\n",
    "            print (len(tweet_list)) #to see that it's progressing and not failed\n",
    "    \n",
    "    #For this next part, I recognize I could have limited the data being pulled, \n",
    "    #but was concerned I would either miss a Sunday of data, which is likely a heavy tweet day for football teams\n",
    "    #or that I wouldnt get the data I wanted while I was still learning the twitter api.  \n",
    "    #So I opted to pull all data and filter it before saving.  \n",
    "    limit_data = get_tweet_data(tweet_list)\n",
    "    save_data = save_tweets(limit_data,filename)\n",
    "    return(save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pulling data I anticipate I'll need from the full set of data pulled from the api \n",
    "def get_tweet_data(tweepy_list):\n",
    "\n",
    "    tweet_data=[]\n",
    "\n",
    "    for tweet in tweepy_list:\n",
    "\n",
    "        current_tweet=dict()\n",
    "        current_tweet['text']=tweet.text\n",
    "        current_tweet['created_at']=tweet.created_at.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        current_tweet['id_str']=tweet.id_str\n",
    "        current_tweet['retweeted']=tweet.retweeted\n",
    "        \n",
    "        user_dict=tweet._json['user']\n",
    "        current_tweet['user_id_str']=user_dict['id_str']\n",
    "        current_tweet['user_location']=user_dict['location']\n",
    "        \n",
    "        entities_dict=tweet._json['entities']\n",
    "        current_tweet['hashtags']=entities_dict['hashtags']\n",
    "                \n",
    "        if tweet.place: \n",
    "            place_dict = tweet._json['place']\n",
    "            current_tweet['place_full_name']=place_dict['full_name']\n",
    "            current_tweet['place_place_type']=place_dict['place_type']\n",
    "            current_tweet['place_name']=place_dict['name']\n",
    "            current_tweet['place_country_code']=place_dict['country_code']\n",
    "            current_tweet['place_country']=place_dict['country']\n",
    "        else:\n",
    "            current_tweet['place_full_name'] = ''\n",
    "            current_tweet['place_place_type']= ''\n",
    "            current_tweet['place_name']= ''\n",
    "            current_tweet['place_country_code']= ''\n",
    "            current_tweet['place_country']= ''\n",
    "        \n",
    "        tweet_data.append(current_tweet)\n",
    "    \n",
    "    return(tweet_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save the tweets to a json file\n",
    "def save_tweets(tweets,filename):\n",
    "    try:\n",
    "        file=open(filename,\"w\")\n",
    "        json.dump(tweets,file)\n",
    "        file.close() \n",
    "        return(\"Save Complete!\")\n",
    "    except:\n",
    "        return(\"Something went wrong, file wasn't saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the functions to pull data**\n",
    "\n",
    "The following were the runs that were completed using the get_tweets_by_hashtag function.  I wanted these separate in order to be able to choose later if I wanted to use them all or not. They are commented out so that they don't replace the files created for analysis while rerunning the entire notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sunday December 7 \n",
    "# get_tweets_by_hashtag('#%23gopackgo', 100000,'tweets_gopackgo_all_12072019.json')\n",
    "# get_tweets_by_hashtag('#%23packernation',100000,'tweets_packernation_all_12072019.json')\n",
    "# get_tweets_by_hashtag('#%23greenandgold ', 100000,'tweets_greenandgold_all_12072019.json')\n",
    "# get_tweets_by_hashtag('#%23greenbaypackers', 100000,'tweets_greenbaypackers_all_12072019.json')\n",
    "# get_tweets_by_hashtag('#%23packers', 100000,'tweets_packers_all_12072019.json')\n",
    "\n",
    "\n",
    "#Tuesday December 10\n",
    "# get_tweets_by_hashtag('#%23gopackgo', 100000,'tweets_gopackgo_all_12102019.json')\n",
    "# get_tweets_by_hashtag('#%23packernation',100000,'tweets_packernation_all_12102019.json')\n",
    "# get_tweets_by_hashtag('#%23greenandgold ', 100000,'tweets_greenandgold_all_12102019.json')\n",
    "# get_tweets_by_hashtag('#%23greenbaypackers', 100000,'tweets_greenbaypackers_all_12102019.json')\n",
    "# get_tweets_by_hashtag('#%23packers', 100000,'tweets_packers_all_12102019.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Cleaning and modifying the data so it can be used in R\n",
    "In this part, I work through merging data previously gathered, parsing for a state to use as a location, removing tweets from WI and duplicates, and performing sentiment analysis.  The last step saves a file to be used in R analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to merge json files created in part 1\n",
    "def merge_json_files(file_name_list):\n",
    "\n",
    "    tweets_list = []\n",
    "\n",
    "    for ii in range(len(file_name_list)):\n",
    "        with open(file_name_list[ii], 'r') as file:\n",
    "            temp_list = json.load(file)\n",
    "            tweets_list = tweets_list + temp_list\n",
    "    return(tweets_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to pull only the fields I want to use from the Twitter data gathered\n",
    "def get_specific_data (tweets_list):\n",
    "    all_tweets_list = []\n",
    "    id_list = []\n",
    "\n",
    "    for ii in range(len(tweets_list)):\n",
    "        text = tweets_list[ii]['text']\n",
    "        id_str = int(tweets_list[ii]['id_str'])\n",
    "        retweet = True if text[:4]=='RT @' else False #didn't pull retweet field, workaround for identifying them\n",
    "        user_location  = tweets_list[ii]['user_location']\n",
    "        place_full_name  = tweets_list[ii]['place_full_name']\n",
    "        location = ''\n",
    "        team_count = ''\n",
    "        sentiment = ''\n",
    "        \n",
    "        #don't add duplicates or retweets to the list\n",
    "        if id_str not in id_list and retweet == False:\n",
    "            all_tweets_list.append([text,id_str,retweet,user_location,place_full_name,location,team_count,sentiment])\n",
    "            id_list.append(id_str)\n",
    "\n",
    "    return(all_tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get location for each tweet based on data in either the user_place field or place_full_name field\n",
    "def get_location_for_tweets (tweet_list):\n",
    "  \n",
    "    states = pd.read_csv(\"states.csv\",na_values='*')\n",
    "    statenames = states[\"State\"].tolist()\n",
    "    statenames = [x.upper() for x in statenames]\n",
    "    stateabbrvs = states[\"Abbreviation\"].tolist()\n",
    "    \n",
    "    #create these to use to do replacements/lookups based on \n",
    "    #state full name (to abbreviation) and abbreviation (for team count)\n",
    "    state_dict = {}\n",
    "    nflteams_dict = {}    \n",
    "    for ii in range(len(states)):\n",
    "        state_dict[states.iloc[ii]['State'].upper()] = states.iloc[ii]['Abbreviation']\n",
    "        nflteams_dict[states.iloc[ii]['Abbreviation']] =states.iloc[ii]['Number of NFL Teams']\n",
    "\n",
    "    #use either the user_location or place_full_name to find state info.  \n",
    "    #Split the fields into word lists, and try to find the full state name or state abbreviation.\n",
    "    for ii in range(len(tweet_list)):\n",
    "        \n",
    "        #create the \"place\" list depending on on which field can be used.\n",
    "        if tweet_list[ii][3] != '':\n",
    "            place = tweet_list[ii][3].split(\",\") \n",
    "        else:\n",
    "            place = tweet_list[ii][4].split(\", \")\n",
    "            \n",
    "        #strip spaces and make upper case for easier comparison   \n",
    "        place = [word.strip() for word in place]\n",
    "        place = [x.upper() for x in place]\n",
    "        \n",
    "        #if the place list contains a value in either the state or abbreviation list, set the location to that place\n",
    "        for jj in range(len(place)):\n",
    "            if place[jj] in statenames or place[jj] in stateabbrvs:\n",
    "                tweet_list[ii][5] = place[jj]\n",
    "                \n",
    "    #make dataframe to simplify dropping data            \n",
    "    tweetsdf = pd.DataFrame(tweet_list, columns = ['text', 'id_str','retweet','user_location',\n",
    "                                                   'place_full_name','location','team_count','sentiment']) \n",
    "\n",
    "    #replace state with abbreviation\n",
    "    tweetsdf ['location'] = tweetsdf['location'].map(state_dict).fillna(tweetsdf['location'])\n",
    "    \n",
    "    #drop tweets without location\n",
    "    tweetsdf = tweetsdf.drop(tweetsdf[tweetsdf['location']==''].index)\n",
    "    \n",
    "    #drop tweets from WI\n",
    "    tweetsdf = tweetsdf.drop(tweetsdf[tweetsdf['location']=='WI'].index)\n",
    "       \n",
    "    #add team count to each tweet\n",
    "    tweetsdf ['team_count'] = tweetsdf['location'].map(nflteams_dict).fillna(tweetsdf['team_count'])\n",
    "    \n",
    "    #change back to list for additional processing\n",
    "    tweets_list = tweetsdf.values.tolist()\n",
    "    \n",
    "    return(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove special characters from tweet text\n",
    "def clean_tweet(tweet_text):\n",
    "    for p in string.punctuation:\n",
    "        tweet_text=tweet_text.replace(p,\"\")\n",
    "        return(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to get tweet sentiment using textblob\n",
    "def get_tweet_sentiment(tweets): \n",
    "    for ii in range(len(tweets)):\n",
    "        analysis = TextBlob(clean_tweet(tweets[ii][0])) \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            tweets[ii][7] = 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            tweets[ii][7] = 'neutral'\n",
    "        else: \n",
    "            tweets[ii][7] = 'negative'\n",
    "    tweetsdf = pd.DataFrame(tweets, columns = ['text', 'id_str','retweet','user_location',\n",
    "                                               'place_full_name','location','team_count','sentiment']) \n",
    "    \n",
    "    #drop neutral sentiment tweets\n",
    "    tweetsdf = tweetsdf.drop(tweetsdf[tweetsdf['sentiment']=='neutral'].index)\n",
    "    return(tweetsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to run the various steps to clean and remove tweets\n",
    "def get_unique_tweets(tweets_list):\n",
    "\n",
    "    #1. Get fields needed for analysis and remove duplicates\n",
    "    specific_tweets = get_specific_data (tweets_list)\n",
    "\n",
    "    #2.Determine location based on other fields in each tweet, remove tweets without location or from WI\n",
    "    location_tweets = get_location_for_tweets(specific_tweets)\n",
    "\n",
    "    #3. Determine sentiment for each tweet and remove neutral tweets\n",
    "    sentiment_tweets = get_tweet_sentiment(location_tweets)\n",
    "\n",
    "    return(sentiment_tweets)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the functions to clean data**\n",
    "\n",
    "This part runs the rest of the functions and save the results to a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2861\n"
     ]
    }
   ],
   "source": [
    "#merge files together that have tweet data\n",
    "file_list = ['tweets_gopackgo_all_12072019.json','tweets_greenbaypackers_all_12072019.json',\n",
    "             'tweets_greenandgold_all_12072019.json','tweets_packernation_all_12072019.json',\n",
    "             'tweets_packers_all_12072019.json','tweets_gopackgo_all_12102019.json',\n",
    "             'tweets_greenbaypackers_all_12102019.json','tweets_greenandgold_all_12102019.json',\n",
    "             'tweets_packernation_all_12102019.json','tweets_packers_all_12102019.json']\n",
    "\n",
    "merged_list = merge_json_files(file_list)\n",
    "\n",
    "#get list of unique tweets with desired information\n",
    "unique_tweets = get_unique_tweets(merged_list)\n",
    "print(len(unique_tweets)) #visual confirmation that process ran and returned expected number of tweets\n",
    "\n",
    "#save lists to csv\n",
    "unique_tweets.to_csv('unique_tweets.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
